{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入成功！\n",
      "可用的类和函数：\n",
      "['Decoder', 'DecoderLayer', 'Encoder', 'EncoderLayer', 'FeedForward', 'LayerNorm', 'MultiHeadAttention', 'PositionalEncoding', 'ScaleDotProductAttention', 'Transformer', 'TransformerEmbedding', 'get_ipython', 'math', 'nn', 'plt', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 从transformer.ipynb中导入transformer类\n",
    "try:\n",
    "    import nbimporter\n",
    "    import transformer as tr\n",
    "    print(\"导入成功！\")\n",
    "    print(\"可用的类和函数：\")\n",
    "    print([name for name in dir(tr) if not name.startswith('_')])\n",
    "except Exception as e:\n",
    "    print(f\"导入失败：{e}\")\n",
    "    print(\"请检查transformer.ipynb是否在当前目录下\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据加载和处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/project/llmProject/my_llm_resource/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/young/project/llmProject/my_llm_resource/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 29000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1014\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "class HFVocabBuilder:\n",
    "    def __init__(self, tokenizer_type='wordlevel'):\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.source_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        \n",
    "    def build_vocab(self, train_data, src=\"de\", trg=\"en\", min_freq=1):\n",
    "        \"\"\"使用HuggingFace tokenizers构建词汇表\"\"\"\n",
    "        \n",
    "        # 提取源语言和目标语言文本\n",
    "        source_texts = []\n",
    "        target_texts = []\n",
    "        \n",
    "        for example in train_data:\n",
    "            source_texts.append(example[src])\n",
    "            target_texts.append(example[trg])\n",
    "        \n",
    "        # 构建源语言tokenizer\n",
    "        self.source_tokenizer = self._build_single_tokenizer(source_texts, min_freq)\n",
    "        \n",
    "        # 构建目标语言tokenizer\n",
    "        self.target_tokenizer = self._build_single_tokenizer(target_texts, min_freq)\n",
    "        \n",
    "        # 创建兼容接口\n",
    "        self.source = self._create_vocab_interface(self.source_tokenizer)\n",
    "        self.target = self._create_vocab_interface(self.target_tokenizer)\n",
    "        \n",
    "        print(f\"源语言词汇表大小: {self.source_tokenizer.get_vocab_size()}\")\n",
    "        print(f\"目标语言词汇表大小: {self.target_tokenizer.get_vocab_size()}\")\n",
    "    \n",
    "    def _build_single_tokenizer(self, texts, min_freq):\n",
    "        \"\"\"构建单个tokenizer\"\"\"\n",
    "        if self.tokenizer_type == 'wordlevel':\n",
    "            # 使用WordLevel模型\n",
    "            tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "            trainer = WordLevelTrainer(\n",
    "                special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    "                min_frequency=min_freq\n",
    "            )\n",
    "        else:\n",
    "            # 使用BPE模型\n",
    "            tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "            trainer = BpeTrainer(\n",
    "                special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    "                min_frequency=min_freq\n",
    "            )\n",
    "        \n",
    "        # 设置预处理器\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # 设置后处理器（添加特殊标记）\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<sos> $A <eos>\",\n",
    "            special_tokens=[(\"<sos>\", 2), (\"<eos>\", 3)]\n",
    "        )\n",
    "        \n",
    "        # 训练tokenizer\n",
    "        tokenizer.train_from_iterator(texts, trainer)\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def _create_vocab_interface(self, tokenizer):\n",
    "        \"\"\"创建与torchtext兼容的接口\"\"\"\n",
    "        vocab_obj = type('Vocab', (), {})()\n",
    "        \n",
    "        # 获取词汇表\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        vocab_obj.stoi = vocab\n",
    "        vocab_obj.itos = {v: k for k, v in vocab.items()}\n",
    "        vocab_obj.__len__ = lambda: len(vocab)\n",
    "        \n",
    "        return type('Field', (), {'vocab': vocab_obj})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源语言词汇表大小: 8060\n",
      "目标语言词汇表大小: 6203\n",
      "0 0 2\n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "hf_builder = HFVocabBuilder()\n",
    "hf_builder.build_vocab(train_data=dataset['train'], src=\"de\", trg=\"en\", min_freq=2)\n",
    "\n",
    "# 获取特殊标记索引\n",
    "src_pad_idx = hf_builder.source.vocab.stoi['<pad>']\n",
    "trg_pad_idx = hf_builder.target.vocab.stoi['<pad>']\n",
    "trg_sos_idx = hf_builder.target.vocab.stoi['<sos>']\n",
    "print(src_pad_idx, trg_pad_idx, trg_sos_idx)\n",
    "\n",
    "enc_voc_size = hf_builder.source_tokenizer.get_vocab_size()\n",
    "dec_voc_size = hf_builder.target_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "tokenized text:\n",
      "[2, 21, 86, 223, 32, 88, 22, 97, 7, 16, 116, 7956, 3260, 4, 3]\n",
      "decoded text:\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche .\n",
      "special tokens:\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "tmp_text = dataset['train'][0]['de']\n",
    "print(\"original text:\")\n",
    "print(tmp_text)\n",
    "print(\"tokenized text:\")\n",
    "tmp_text_encoded = hf_builder.source_tokenizer.encode(tmp_text).ids \n",
    "print(tmp_text_encoded)\n",
    "print(\"decoded text:\")\n",
    "print(hf_builder.source_tokenizer.decode(tmp_text_encoded))\n",
    "print(\"special tokens:\")\n",
    "print(hf_builder.source_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<unk>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<sos>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<eos>\", add_special_tokens=False).ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    src_name,\n",
    "    trg_name,\n",
    "    src_tokenizer,\n",
    "    trg_tokenizer,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到批次中最长的序列\n",
    "    batch_max_length = max(max(len(item[src_name]), len(item[trg_name]))+2 for item in batch)\n",
    "\n",
    "    # 填充并准备输入和目标\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        # 使用tokenizer进行tokenize\n",
    "        item[src_name] = src_tokenizer.encode(item[src_name]).ids\n",
    "        item[trg_name] = trg_tokenizer.encode(item[trg_name]).ids\n",
    "        # 填充到最大长度\n",
    "        item[src_name] = item[src_name] + src_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids * (batch_max_length - len(item[src_name]))\n",
    "        item[trg_name] = item[trg_name] + trg_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids * (batch_max_length - len(item[trg_name]))\n",
    "\n",
    "        inputs_lst.append(torch.tensor(item[src_name]))\n",
    "        targets_lst.append(torch.tensor(item[trg_name]))\n",
    "\n",
    "    # 将输入和目标的列表转换为张量，并转移到目标设备\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return {\n",
    "        src_name: inputs_tensor,\n",
    "        trg_name: targets_tensor\n",
    "    }\n",
    "\n",
    "class HuggingFaceMulti30k:\n",
    "    def __init__(self, dataset, batch_size=32, tokenizer=None, src_name=\"de\", trg_name=\"en\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_name = src_name\n",
    "        self.trg_name = trg_name\n",
    "        \n",
    "    def get_dataloaders(self):\n",
    "        my_collate_fn = lambda x: custom_collate_fn(x, self.src_name, self.trg_name, self.tokenizer.source_tokenizer, self.tokenizer.target_tokenizer)\n",
    "        train_loader = DataLoader(\n",
    "            self.dataset['train'], \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.dataset['validation'], \n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.dataset['test'], \n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2,   5,  12,   9,  18,  17, 276,  25,   6,   1,   7, 548,  66,   4,\n",
      "          3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n",
      "Ein Mann und eine Frau springen von einem in blaues Wasser .\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = HuggingFaceMulti30k(dataset, batch_size=128, tokenizer=hf_builder, src_name=\"de\", trg_name=\"en\")\n",
    "train_loader, valid_loader, test_loader = dataset_loader.get_dataloaders()\n",
    "\n",
    "for batch in train_loader:\n",
    "    test_batch_sample = batch[\"de\"][0]\n",
    "    print(test_batch_sample)\n",
    "    print(hf_builder.source_tokenizer.decode(test_batch_sample.numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model parameter setting\n",
    "batch_size = 128\n",
    "max_len = 256\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 10\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 54,623,291 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/h44tfn_x35jg4s7msmb1gdzw0000gp/T/ipykernel_68982/3565715656.py:22: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(8060, 512)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(6203, 512)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (cross_att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (out_proj): Linear(in_features=512, out_features=6203, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "from bleu import get_bleu\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "\n",
    "model = tr.Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_heads=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, src_name, trg_name):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[src_name]\n",
    "        trg = batch[trg_name]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tokenizer, src_name, trg_name):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[src_name]\n",
    "            trg = batch[trg_name]\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            total_bleu = []\n",
    "            for j in range(trg.shape[0]):\n",
    "                # try:\n",
    "                trg_words = tokenizer.target_tokenizer.decode(trg[j].numpy())\n",
    "                output_words = output[j].max(dim=1)[1]\n",
    "                output_words = tokenizer.target_tokenizer.decode(output_words.numpy())\n",
    "                bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n",
    "                total_bleu.append(bleu)\n",
    "                # except:\n",
    "                #     pass\n",
    "\n",
    "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            batch_bleu.append(total_bleu)\n",
    "\n",
    "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len(iterator), batch_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 9.853903770446777\n",
      "step : 0.44 % , loss : 9.524776458740234\n",
      "step : 0.88 % , loss : 9.280054092407227\n",
      "step : 1.32 % , loss : 8.981889724731445\n",
      "step : 1.76 % , loss : 8.745308876037598\n",
      "step : 2.2 % , loss : 8.58002758026123\n",
      "step : 2.64 % , loss : 8.390743255615234\n",
      "step : 3.08 % , loss : 8.256885528564453\n",
      "step : 3.52 % , loss : 8.201362609863281\n",
      "step : 3.96 % , loss : 8.092958450317383\n",
      "step : 4.41 % , loss : 8.029891014099121\n",
      "step : 4.85 % , loss : 7.836681842803955\n",
      "step : 5.29 % , loss : 7.888801097869873\n",
      "step : 5.73 % , loss : 7.890926361083984\n",
      "step : 6.17 % , loss : 7.708928108215332\n",
      "step : 6.61 % , loss : 7.718955039978027\n",
      "step : 7.05 % , loss : 7.72428035736084\n",
      "step : 7.49 % , loss : 7.534897804260254\n",
      "step : 7.93 % , loss : 7.585045337677002\n",
      "step : 8.37 % , loss : 7.573445796966553\n",
      "step : 8.81 % , loss : 7.541711330413818\n",
      "step : 9.25 % , loss : 7.4730963706970215\n",
      "step : 9.69 % , loss : 7.40364933013916\n",
      "step : 10.13 % , loss : 7.339963912963867\n",
      "step : 10.57 % , loss : 7.427947998046875\n",
      "step : 11.01 % , loss : 7.346148490905762\n",
      "step : 11.45 % , loss : 7.31271505355835\n",
      "step : 11.89 % , loss : 7.176056861877441\n",
      "step : 12.33 % , loss : 7.314685821533203\n",
      "step : 12.78 % , loss : 7.264922142028809\n",
      "step : 13.22 % , loss : 7.153938293457031\n",
      "step : 13.66 % , loss : 7.04772424697876\n",
      "step : 14.1 % , loss : 7.167598724365234\n",
      "step : 14.54 % , loss : 7.095455646514893\n",
      "step : 14.98 % , loss : 7.149840831756592\n",
      "step : 15.42 % , loss : 7.088624954223633\n",
      "step : 15.86 % , loss : 7.1123576164245605\n",
      "step : 16.3 % , loss : 6.968991279602051\n",
      "step : 16.74 % , loss : 6.9436821937561035\n",
      "step : 17.18 % , loss : 7.026702404022217\n",
      "step : 17.62 % , loss : 6.990933895111084\n",
      "step : 18.06 % , loss : 7.041772842407227\n",
      "step : 18.5 % , loss : 6.949833393096924\n",
      "step : 18.94 % , loss : 7.011204242706299\n",
      "step : 19.38 % , loss : 6.988320350646973\n",
      "step : 19.82 % , loss : 6.874426364898682\n",
      "step : 20.26 % , loss : 6.789316177368164\n",
      "step : 20.7 % , loss : 6.8808088302612305\n",
      "step : 21.15 % , loss : 6.827695846557617\n",
      "step : 21.59 % , loss : 6.832178115844727\n",
      "step : 22.03 % , loss : 6.846177101135254\n",
      "step : 22.47 % , loss : 6.8093342781066895\n",
      "step : 22.91 % , loss : 6.8235368728637695\n",
      "step : 23.35 % , loss : 6.738094329833984\n",
      "step : 23.79 % , loss : 6.719572067260742\n",
      "step : 24.23 % , loss : 6.719545841217041\n",
      "step : 24.67 % , loss : 6.75389289855957\n",
      "step : 25.11 % , loss : 6.741334438323975\n",
      "step : 25.55 % , loss : 6.795535564422607\n",
      "step : 25.99 % , loss : 6.692773818969727\n",
      "step : 26.43 % , loss : 6.728715419769287\n",
      "step : 26.87 % , loss : 6.722584247589111\n",
      "step : 27.31 % , loss : 6.601239204406738\n",
      "step : 27.75 % , loss : 6.615918159484863\n",
      "step : 28.19 % , loss : 6.650510311126709\n",
      "step : 28.63 % , loss : 6.762027263641357\n",
      "step : 29.07 % , loss : 6.650981903076172\n",
      "step : 29.52 % , loss : 6.6983323097229\n",
      "step : 29.96 % , loss : 6.693567276000977\n",
      "step : 30.4 % , loss : 6.758042335510254\n",
      "step : 30.84 % , loss : 6.711455821990967\n",
      "step : 31.28 % , loss : 6.6848835945129395\n",
      "step : 31.72 % , loss : 6.667666912078857\n",
      "step : 32.16 % , loss : 6.68060827255249\n",
      "step : 32.6 % , loss : 6.653868198394775\n",
      "step : 33.04 % , loss : 6.640863418579102\n",
      "step : 33.48 % , loss : 6.635881423950195\n",
      "step : 33.92 % , loss : 6.542173385620117\n",
      "step : 34.36 % , loss : 6.561259746551514\n",
      "step : 34.8 % , loss : 6.54119348526001\n",
      "step : 35.24 % , loss : 6.589405059814453\n",
      "step : 35.68 % , loss : 6.4572577476501465\n",
      "step : 36.12 % , loss : 6.562417507171631\n",
      "step : 36.56 % , loss : 6.560286045074463\n",
      "step : 37.0 % , loss : 6.583051681518555\n",
      "step : 37.44 % , loss : 6.524348258972168\n",
      "step : 37.89 % , loss : 6.570738792419434\n",
      "step : 38.33 % , loss : 6.583096504211426\n",
      "step : 38.77 % , loss : 6.589425563812256\n",
      "step : 39.21 % , loss : 6.437729358673096\n",
      "step : 39.65 % , loss : 6.489462375640869\n",
      "step : 40.09 % , loss : 6.536159038543701\n",
      "step : 40.53 % , loss : 6.460356712341309\n",
      "step : 40.97 % , loss : 6.45939826965332\n",
      "step : 41.41 % , loss : 6.50677490234375\n",
      "step : 41.85 % , loss : 6.515608310699463\n",
      "step : 42.29 % , loss : 6.464657306671143\n",
      "step : 42.73 % , loss : 6.453577041625977\n",
      "step : 43.17 % , loss : 6.427153587341309\n",
      "step : 43.61 % , loss : 6.602017879486084\n",
      "step : 44.05 % , loss : 6.460774898529053\n",
      "step : 44.49 % , loss : 6.4956560134887695\n",
      "step : 44.93 % , loss : 6.401331901550293\n",
      "step : 45.37 % , loss : 6.39070987701416\n",
      "step : 45.81 % , loss : 6.472883701324463\n",
      "step : 46.26 % , loss : 6.449930667877197\n",
      "step : 46.7 % , loss : 6.4941229820251465\n",
      "step : 47.14 % , loss : 6.4582929611206055\n",
      "step : 47.58 % , loss : 6.436972618103027\n",
      "step : 48.02 % , loss : 6.428132057189941\n",
      "step : 48.46 % , loss : 6.503166675567627\n",
      "step : 48.9 % , loss : 6.592360496520996\n",
      "step : 49.34 % , loss : 6.456882953643799\n",
      "step : 49.78 % , loss : 6.381217956542969\n",
      "step : 50.22 % , loss : 6.508114337921143\n",
      "step : 50.66 % , loss : 6.373149871826172\n",
      "step : 51.1 % , loss : 6.382617473602295\n",
      "step : 51.54 % , loss : 6.425484657287598\n",
      "step : 51.98 % , loss : 6.358447074890137\n",
      "step : 52.42 % , loss : 6.443879127502441\n",
      "step : 52.86 % , loss : 6.445169448852539\n",
      "step : 53.3 % , loss : 6.372280120849609\n",
      "step : 53.74 % , loss : 6.358924388885498\n",
      "step : 54.19 % , loss : 6.39573335647583\n",
      "step : 54.63 % , loss : 6.300100803375244\n",
      "step : 55.07 % , loss : 6.312001705169678\n",
      "step : 55.51 % , loss : 6.414970874786377\n",
      "step : 55.95 % , loss : 6.437667369842529\n",
      "step : 56.39 % , loss : 6.3665666580200195\n",
      "step : 56.83 % , loss : 6.368130683898926\n",
      "step : 57.27 % , loss : 6.45005989074707\n",
      "step : 57.71 % , loss : 6.463771820068359\n",
      "step : 58.15 % , loss : 6.242249488830566\n",
      "step : 58.59 % , loss : 6.352172374725342\n",
      "step : 59.03 % , loss : 6.306352615356445\n",
      "step : 59.47 % , loss : 6.433151721954346\n",
      "step : 59.91 % , loss : 6.244969367980957\n",
      "step : 60.35 % , loss : 6.355404376983643\n",
      "step : 60.79 % , loss : 6.354061603546143\n",
      "step : 61.23 % , loss : 6.358839511871338\n",
      "step : 61.67 % , loss : 6.316441535949707\n",
      "step : 62.11 % , loss : 6.291806221008301\n",
      "step : 62.56 % , loss : 6.31832218170166\n",
      "step : 63.0 % , loss : 6.279744625091553\n",
      "step : 63.44 % , loss : 6.4561381340026855\n",
      "step : 63.88 % , loss : 6.311263084411621\n",
      "step : 64.32 % , loss : 6.2876362800598145\n",
      "step : 64.76 % , loss : 6.272156715393066\n",
      "step : 65.2 % , loss : 6.301854610443115\n",
      "step : 65.64 % , loss : 6.327695369720459\n",
      "step : 66.08 % , loss : 6.209679126739502\n",
      "step : 66.52 % , loss : 6.350154876708984\n",
      "step : 66.96 % , loss : 6.341007232666016\n",
      "step : 67.4 % , loss : 6.234334945678711\n",
      "step : 67.84 % , loss : 6.264979362487793\n",
      "step : 68.28 % , loss : 6.261865615844727\n",
      "step : 68.72 % , loss : 6.194599151611328\n",
      "step : 69.16 % , loss : 6.250604152679443\n",
      "step : 69.6 % , loss : 6.272560119628906\n",
      "step : 70.04 % , loss : 6.266604423522949\n",
      "step : 70.48 % , loss : 6.2752203941345215\n",
      "step : 70.93 % , loss : 6.265048503875732\n",
      "step : 71.37 % , loss : 6.1513352394104\n",
      "step : 71.81 % , loss : 6.309045791625977\n",
      "step : 72.25 % , loss : 6.197413444519043\n",
      "step : 72.69 % , loss : 6.217245578765869\n",
      "step : 73.13 % , loss : 6.352328777313232\n",
      "step : 73.57 % , loss : 6.284008502960205\n",
      "step : 74.01 % , loss : 6.2217278480529785\n",
      "step : 74.45 % , loss : 6.232237815856934\n",
      "step : 74.89 % , loss : 6.2073493003845215\n",
      "step : 75.33 % , loss : 6.217281818389893\n",
      "step : 75.77 % , loss : 6.211670398712158\n",
      "step : 76.21 % , loss : 6.29902458190918\n",
      "step : 76.65 % , loss : 6.158501625061035\n",
      "step : 77.09 % , loss : 6.223654270172119\n",
      "step : 77.53 % , loss : 6.233151912689209\n",
      "step : 77.97 % , loss : 6.2623372077941895\n",
      "step : 78.41 % , loss : 6.306135654449463\n",
      "step : 78.85 % , loss : 6.150400161743164\n",
      "step : 79.3 % , loss : 6.208990573883057\n",
      "step : 79.74 % , loss : 6.221591472625732\n",
      "step : 80.18 % , loss : 6.171437740325928\n",
      "step : 80.62 % , loss : 6.212405681610107\n",
      "step : 81.06 % , loss : 6.124265670776367\n",
      "step : 81.5 % , loss : 6.085323810577393\n",
      "step : 81.94 % , loss : 6.1698222160339355\n",
      "step : 82.38 % , loss : 6.155375957489014\n",
      "step : 82.82 % , loss : 6.221286773681641\n",
      "step : 83.26 % , loss : 6.275336265563965\n",
      "step : 83.7 % , loss : 6.298095226287842\n",
      "step : 84.14 % , loss : 6.100185394287109\n",
      "step : 84.58 % , loss : 6.22377872467041\n",
      "step : 85.02 % , loss : 6.13240385055542\n",
      "step : 85.46 % , loss : 6.139665126800537\n",
      "step : 85.9 % , loss : 6.220315933227539\n",
      "step : 86.34 % , loss : 6.085900783538818\n",
      "step : 86.78 % , loss : 6.273918151855469\n",
      "step : 87.22 % , loss : 6.112486362457275\n",
      "step : 87.67 % , loss : 6.22231388092041\n",
      "step : 88.11 % , loss : 6.178821563720703\n",
      "step : 88.55 % , loss : 6.256650924682617\n",
      "step : 88.99 % , loss : 6.103405475616455\n",
      "step : 89.43 % , loss : 6.137348651885986\n",
      "step : 89.87 % , loss : 6.169150352478027\n",
      "step : 90.31 % , loss : 6.096781253814697\n",
      "step : 90.75 % , loss : 6.107442855834961\n",
      "step : 91.19 % , loss : 6.186844348907471\n",
      "step : 91.63 % , loss : 6.09834098815918\n",
      "step : 92.07 % , loss : 6.124290466308594\n",
      "step : 92.51 % , loss : 6.115556716918945\n",
      "step : 92.95 % , loss : 6.167496681213379\n",
      "step : 93.39 % , loss : 6.108730792999268\n",
      "step : 93.83 % , loss : 6.064192771911621\n",
      "step : 94.27 % , loss : 6.096368312835693\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "train_losses, test_losses, bleus = [], [], []\n",
    "for step in range(total_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, src_name=\"de\", trg_name=\"en\")\n",
    "    valid_loss, bleu = evaluate(model, valid_loader, criterion, hf_builder, src_name=\"de\", trg_name=\"en\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    if step > warmup:\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    bleus.append(bleu)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    os.makedirs(\"saved\", exist_ok=True)\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
    "\n",
    "    os.makedirs(\"result\", exist_ok=True)\n",
    "    f = open('result/train_loss.txt', 'w')\n",
    "    f.write(str(train_losses))\n",
    "    f.close()\n",
    "\n",
    "    f = open('result/bleu.txt', 'w')\n",
    "    f.write(str(bleus))\n",
    "    f.close()\n",
    "\n",
    "    f = open('result/test_loss.txt', 'w')\n",
    "    f.write(str(test_losses))\n",
    "    f.close()\n",
    "\n",
    "    print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\tBLEU Score: {bleu:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
