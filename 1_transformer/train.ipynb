{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入成功！\n",
      "可用的类和函数：\n",
      "['Decoder', 'DecoderLayer', 'Encoder', 'EncoderLayer', 'FeedForward', 'LayerNorm', 'MultiHeadAttention', 'PositionalEncoding', 'ScaleDotProductAttention', 'Transformer', 'TransformerEmbedding', 'get_ipython', 'math', 'nn', 'plt', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 从transformer.ipynb中导入transformer类\n",
    "try:\n",
    "    import nbimporter\n",
    "    import transformer as tr\n",
    "    print(\"导入成功！\")\n",
    "    print(\"可用的类和函数：\")\n",
    "    print([name for name in dir(tr) if not name.startswith('_')])\n",
    "except Exception as e:\n",
    "    print(f\"导入失败：{e}\")\n",
    "    print(\"请检查transformer.ipynb是否在当前目录下\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据加载和处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 29000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1014\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "class HFVocabBuilder:\n",
    "    def __init__(self, tokenizer_type='wordlevel'):\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.source_tokenizer = None\n",
    "        self.target_tokenizer = None\n",
    "        \n",
    "    def build_vocab(self, train_data, src=\"de\", trg=\"en\", min_freq=1):\n",
    "        \"\"\"使用HuggingFace tokenizers构建词汇表\"\"\"\n",
    "        \n",
    "        # 提取源语言和目标语言文本\n",
    "        source_texts = []\n",
    "        target_texts = []\n",
    "        \n",
    "        for example in train_data:\n",
    "            source_texts.append(example[src])\n",
    "            target_texts.append(example[trg])\n",
    "        \n",
    "        # 构建源语言tokenizer\n",
    "        self.source_tokenizer = self._build_single_tokenizer(source_texts, min_freq)\n",
    "        \n",
    "        # 构建目标语言tokenizer\n",
    "        self.target_tokenizer = self._build_single_tokenizer(target_texts, min_freq)\n",
    "        \n",
    "        # 创建兼容接口\n",
    "        self.source = self._create_vocab_interface(self.source_tokenizer)\n",
    "        self.target = self._create_vocab_interface(self.target_tokenizer)\n",
    "        \n",
    "        print(f\"源语言词汇表大小: {self.source_tokenizer.get_vocab_size()}\")\n",
    "        print(f\"目标语言词汇表大小: {self.target_tokenizer.get_vocab_size()}\")\n",
    "    \n",
    "    def _build_single_tokenizer(self, texts, min_freq):\n",
    "        \"\"\"构建单个tokenizer\"\"\"\n",
    "        if self.tokenizer_type == 'wordlevel':\n",
    "            # 使用WordLevel模型\n",
    "            tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "            trainer = WordLevelTrainer(\n",
    "                special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    "                min_frequency=min_freq\n",
    "            )\n",
    "        else:\n",
    "            # 使用BPE模型\n",
    "            tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "            trainer = BpeTrainer(\n",
    "                special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    "                min_frequency=min_freq\n",
    "            )\n",
    "        \n",
    "        # 设置预处理器\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "        # 设置后处理器（添加特殊标记）\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<sos> $A <eos>\",\n",
    "            special_tokens=[(\"<sos>\", 2), (\"<eos>\", 3)]\n",
    "        )\n",
    "        \n",
    "        # 训练tokenizer\n",
    "        tokenizer.train_from_iterator(texts, trainer)\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def _create_vocab_interface(self, tokenizer):\n",
    "        \"\"\"创建与torchtext兼容的接口\"\"\"\n",
    "        vocab_obj = type('Vocab', (), {})()\n",
    "        \n",
    "        # 获取词汇表\n",
    "        vocab = tokenizer.get_vocab()\n",
    "        vocab_obj.stoi = vocab\n",
    "        vocab_obj.itos = {v: k for k, v in vocab.items()}\n",
    "        vocab_obj.__len__ = lambda: len(vocab)\n",
    "        \n",
    "        return type('Field', (), {'vocab': vocab_obj})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源语言词汇表大小: 8060\n",
      "目标语言词汇表大小: 6203\n",
      "0 0 2\n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "hf_builder = HFVocabBuilder()\n",
    "hf_builder.build_vocab(train_data=dataset['train'], src=\"de\", trg=\"en\", min_freq=2)\n",
    "\n",
    "# 获取特殊标记索引\n",
    "src_pad_idx = hf_builder.source.vocab.stoi['<pad>']\n",
    "trg_pad_idx = hf_builder.target.vocab.stoi['<pad>']\n",
    "trg_sos_idx = hf_builder.target.vocab.stoi['<sos>']\n",
    "print(src_pad_idx, trg_pad_idx, trg_sos_idx)\n",
    "\n",
    "enc_voc_size = hf_builder.source_tokenizer.get_vocab_size()\n",
    "dec_voc_size = hf_builder.target_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "tokenized text:\n",
      "[2, 21, 86, 223, 32, 88, 22, 97, 7, 16, 116, 7956, 3260, 4, 3]\n",
      "decoded text:\n",
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche .\n",
      "special tokens:\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "tmp_text = dataset['train'][0]['de']\n",
    "print(\"original text:\")\n",
    "print(tmp_text)\n",
    "print(\"tokenized text:\")\n",
    "tmp_text_encoded = hf_builder.source_tokenizer.encode(tmp_text).ids \n",
    "print(tmp_text_encoded)\n",
    "print(\"decoded text:\")\n",
    "print(hf_builder.source_tokenizer.decode(tmp_text_encoded))\n",
    "print(\"special tokens:\")\n",
    "print(hf_builder.source_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<unk>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<sos>\", add_special_tokens=False).ids)\n",
    "print(hf_builder.source_tokenizer.encode(\"<eos>\", add_special_tokens=False).ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    src_name,\n",
    "    trg_name,\n",
    "    src_tokenizer,\n",
    "    trg_tokenizer,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # 找到批次中最长的序列\n",
    "    batch_max_length = max(max(len(item[src_name]), len(item[trg_name]))+2 for item in batch)\n",
    "\n",
    "    # 填充并准备输入和目标\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        # 使用tokenizer进行tokenize\n",
    "        item[src_name] = src_tokenizer.encode(item[src_name]).ids\n",
    "        item[trg_name] = trg_tokenizer.encode(item[trg_name]).ids\n",
    "        # 填充到最大长度\n",
    "        item[src_name] = item[src_name] + src_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids * (batch_max_length - len(item[src_name]))\n",
    "        item[trg_name] = item[trg_name] + trg_tokenizer.encode(\"<pad>\", add_special_tokens=False).ids * (batch_max_length - len(item[trg_name]))\n",
    "\n",
    "        inputs_lst.append(torch.tensor(item[src_name]))\n",
    "        targets_lst.append(torch.tensor(item[trg_name]))\n",
    "\n",
    "    # 将输入和目标的列表转换为张量，并转移到目标设备\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return {\n",
    "        src_name: inputs_tensor,\n",
    "        trg_name: targets_tensor\n",
    "    }\n",
    "\n",
    "class HuggingFaceMulti30k:\n",
    "    def __init__(self, dataset, batch_size=32, tokenizer=None, src_name=\"de\", trg_name=\"en\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_name = src_name\n",
    "        self.trg_name = trg_name\n",
    "        \n",
    "    def get_dataloaders(self):\n",
    "        my_collate_fn = lambda x: custom_collate_fn(x, self.src_name, self.trg_name, self.tokenizer.source_tokenizer, self.tokenizer.target_tokenizer)\n",
    "        train_loader = DataLoader(\n",
    "            self.dataset['train'], \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.dataset['validation'], \n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.dataset['test'], \n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=my_collate_fn\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2,  14,  17,   7,  13,  48,  94,  31, 144,  52,   6,  90,   4,   3,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = HuggingFaceMulti30k(dataset, batch_size=128, tokenizer=hf_builder, src_name=\"de\", trg_name=\"en\")\n",
    "train_loader, valid_loader, test_loader = dataset_loader.get_dataloaders()\n",
    "\n",
    "for batch in train_loader:\n",
    "    test_batch_sample = batch[\"de\"][0]\n",
    "    print(test_batch_sample)\n",
    "    print(hf_builder.source_tokenizer.decode([test_batch_sample[0].numpy()]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model parameter setting\n",
    "batch_size = 128\n",
    "max_len = 256\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 10\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 54,623,291 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7r/h44tfn_x35jg4s7msmb1gdzw0000gp/T/ipykernel_72147/3565715656.py:22: FutureWarning: `nn.init.kaiming_uniform` is now deprecated in favor of `nn.init.kaiming_uniform_`.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(8060, 512)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (token_emb): Embedding(6203, 512)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (cross_att): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention()\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (out_proj): Linear(in_features=512, out_features=6203, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "from bleu import get_bleu\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "\n",
    "\n",
    "model = tr.Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_heads=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, src_name, trg_name):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[src_name]\n",
    "        trg = batch[trg_name]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "        # for debug\n",
    "        # break\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tokenizer, src_name, trg_name):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    batch_bleu = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[src_name]\n",
    "            trg = batch[trg_name]\n",
    "            output = model(src, trg[:, :-1])\n",
    "            # batch_size, trg_len - 1, vocab_size -> batch_size * (trg_len - 1), vocab_size\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            # batch_size, trg_len - 1 -> batch_size * (trg_len - 1)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            total_bleu = []\n",
    "            # print(f\"batch {i}\")\n",
    "            # print(f\"src shape: {src.shape}\")\n",
    "            # print(f\"trg shape: {trg.shape}\")\n",
    "            # print(f\"output shape: {output.shape}\")\n",
    "            # print(f\"output shape: {output_reshape.shape}\")\n",
    "            for j in range(batch[trg_name].shape[0]):\n",
    "                # try:\n",
    "                trg_words = tokenizer.target_tokenizer.decode(batch[trg_name][j].numpy())\n",
    "                output_words = output[j].max(dim=1)[1]\n",
    "                output_words = tokenizer.target_tokenizer.decode(output_words.numpy())\n",
    "                bleu = get_bleu(hypotheses=output_words.split(), reference=trg_words.split())\n",
    "                total_bleu.append(bleu)\n",
    "                # except:\n",
    "                #     pass\n",
    "\n",
    "            total_bleu = sum(total_bleu) / len(total_bleu)\n",
    "            batch_bleu.append(total_bleu)\n",
    "\n",
    "    batch_bleu = sum(batch_bleu) / len(batch_bleu)\n",
    "    return epoch_loss / len(iterator), batch_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0.0 % , loss : 9.852019309997559\n",
      "Epoch: 1 | Time: 0m 38s\n",
      "\tTrain Loss: 0.043 | Train PPL:   1.044\n",
      "\tVal Loss: 9.495 |  Val PPL: 13295.891\n",
      "\tBLEU Score: 0.029\n",
      "step : 0.0 % , loss : 9.52609634399414\n",
      "Epoch: 2 | Time: 0m 32s\n",
      "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
      "\tVal Loss: 9.120 |  Val PPL: 9133.191\n",
      "\tBLEU Score: 0.009\n",
      "step : 0.0 % , loss : 9.236044883728027\n",
      "Epoch: 3 | Time: 0m 35s\n",
      "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
      "\tVal Loss: 8.796 |  Val PPL: 6607.358\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 9.070938110351562\n",
      "Epoch: 4 | Time: 0m 32s\n",
      "\tTrain Loss: 0.040 | Train PPL:   1.041\n",
      "\tVal Loss: 8.534 |  Val PPL: 5087.274\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.722808837890625\n",
      "Epoch: 5 | Time: 0m 35s\n",
      "\tTrain Loss: 0.038 | Train PPL:   1.039\n",
      "\tVal Loss: 8.327 |  Val PPL: 4132.746\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.617325782775879\n",
      "Epoch: 6 | Time: 0m 34s\n",
      "\tTrain Loss: 0.038 | Train PPL:   1.039\n",
      "\tVal Loss: 8.164 |  Val PPL: 3510.891\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.398893356323242\n",
      "Epoch: 7 | Time: 0m 38s\n",
      "\tTrain Loss: 0.037 | Train PPL:   1.038\n",
      "\tVal Loss: 8.043 |  Val PPL: 3111.383\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.28788948059082\n",
      "Epoch: 8 | Time: 0m 36s\n",
      "\tTrain Loss: 0.037 | Train PPL:   1.037\n",
      "\tVal Loss: 7.940 |  Val PPL: 2806.005\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.231874465942383\n",
      "Epoch: 9 | Time: 0m 39s\n",
      "\tTrain Loss: 0.036 | Train PPL:   1.037\n",
      "\tVal Loss: 7.847 |  Val PPL: 2558.693\n",
      "\tBLEU Score: 0.000\n",
      "step : 0.0 % , loss : 8.072601318359375\n",
      "Epoch: 10 | Time: 0m 36s\n",
      "\tTrain Loss: 0.036 | Train PPL:   1.036\n",
      "\tVal Loss: 7.768 |  Val PPL: 2363.395\n",
      "\tBLEU Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "total_epoch = 1\n",
    "best_loss = float('inf')\n",
    "\n",
    "train_losses, test_losses, bleus = [], [], []\n",
    "for step in range(total_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, src_name=\"de\", trg_name=\"en\")\n",
    "    valid_loss, bleu = evaluate(model, valid_loader, criterion, hf_builder, src_name=\"de\", trg_name=\"en\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    if step > warmup:\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    bleus.append(bleu)\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # os.makedirs(\"saved\", exist_ok=True)\n",
    "    # if valid_loss < best_loss:\n",
    "    #     best_loss = valid_loss\n",
    "    #     torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(valid_loss))\n",
    "\n",
    "    os.makedirs(\"result\", exist_ok=True)\n",
    "    f = open('result/train_loss.txt', 'w')\n",
    "    f.write(str(train_losses))\n",
    "    f.close()\n",
    "\n",
    "    f = open('result/bleu.txt', 'w')\n",
    "    f.write(str(bleus))\n",
    "    f.close()\n",
    "\n",
    "    f = open('result/test_loss.txt', 'w')\n",
    "    f.write(str(test_losses))\n",
    "    f.close()\n",
    "\n",
    "    print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\tBLEU Score: {bleu:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
